---
id: "Phase 1–4 Testing Strategy"
type: "concept"
related: ["Testable Hypothesis", "Compliance Gap Metric", "HHH Benchmarks", "Circuit Analysis", "Mechanistic Interpretability Verification", "Theological Finetuning"]
---

# Phase 1–4 Testing Strategy

The graduated empirical testing framework for verifying ST effectiveness:

**Phase 1: Fine-tuning alignment.** Initial SDF (Synthetic Document Finetuning) and RLHF training to imprint ST concepts. Success measured by deception benchmarks and compliance gap.

**Phase 2: Mechanistic interpretability.** Using [[Circuit Analysis|circuit analysis]] and probing classifiers to verify ST forms stable, dedicated neural circuits — not surface-level pattern matching.

**Phase 3: Constitutional AI integration.** Embedding ST principles as constitutional constraints and testing their robustness under adversarial scenarios and distribution shifts.

**Phase 4: Pre-training foundations.** Incorporating ST into the pre-training corpus from the ground up, creating the deepest possible internalisation.

Success metrics: compliance gap approaching zero, improved deception benchmarks, strategic alignment in novel scenarios, and consistent reasoning transparency.

ST usage: The planned experimental programme proving or disproving ST's practical alignment value.
Religious parallel: "Test the spirits to see whether they are from God" (1 John 4:1) — empirical verification of spiritual claims.

